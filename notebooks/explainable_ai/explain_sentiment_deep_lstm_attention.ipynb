{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a98b96-ef38-4f8b-9030-e31b094e0d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.13.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 10:12:16.010643: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers\n",
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, Regex\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6315b2e-3b1e-491f-87aa-1ed1e0eada8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "def str_to_list(x):\n",
    "    try:\n",
    "        return list(ast.literal_eval(x))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def string_to_list_dataframe(df):\n",
    "    columns = df.columns.tolist()\n",
    "    columns_w_lists = []\n",
    "    for column in columns:\n",
    "        if df[column].astype(str). \\\n",
    "                apply(lambda x: x.startswith('[') and x.endswith(']')) \\\n",
    "                .astype(int).mean() > 0.8:\n",
    "            columns_w_lists.append(column)\n",
    "    for column in columns_w_lists:\n",
    "        df[column] = df[column].apply(lambda x: str_to_list(x))\n",
    "        df = df[~df[column].isna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f328be8-1484-48c4-8c9e-646217c0e693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_rating(ratings):\n",
    "    mean_rating = np.mean(ratings)\n",
    "    if mean_rating<3:\n",
    "        return '<3'\n",
    "    elif 4>mean_rating>=3:\n",
    "        return '3'\n",
    "    elif  mean_rating>=4:\n",
    "        return '>3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a0caf-2bf8-49b5-8541-62d02d59aaab",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e196439f-3e57-49b4-b258-e513973254a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/user/files_for_research_Vova/processed_data.csv',\\\n",
    "                 usecols=['review_translate','entity_name',\n",
    "                                                            'dataset_name',\n",
    "                                                            'rating',\n",
    "                                                           'translated'])\n",
    "subsets = pd.read_csv('/home/user/files_for_research_Vova/train_val_test_indices.csv')\n",
    "subsets = subsets.merge(df[['dataset_name', 'translated']], left_on='index', right_index=True)\n",
    "bad_indices = pd.read_csv('/home/user/files_for_research_Vova/files_to_check.csv')\n",
    "subsets = subsets[~subsets.index.isin(bad_indices['id'].values)]\n",
    "df = df[~df.index.isin(bad_indices['id'].values)]\n",
    "df, subsets = df.reset_index().drop(columns='index'), subsets.reset_index().drop(columns='index')\n",
    "df['sentiment'] = df['rating'].astype(int).map({1:'negative', 2 : 'negative', \n",
    "                                          3 : 'neutral', 4 : 'positive',\n",
    "                                          5 : 'positive'})\n",
    "mapping = dict([(i,c) for c,i in enumerate(df['sentiment'].unique())])\n",
    "inverse_mapping = dict([(v,k) for k,v in mapping.items()])\n",
    "df['review_translate'] = df['review_translate'].str.lower()\n",
    "if not os.path.exists('/home/user/jupyter_notebooks/df_for_ex_ai.csv'):\n",
    "    df = df[subsets['split']=='test']\n",
    "    df = df.groupby(['dataset_name','entity_name'], as_index=False).agg({'rating' : list,\n",
    "                                                   'review_translate':list\n",
    "                                                   })\n",
    "    df = df[df['rating'].apply(len)>10]\n",
    "    df['mapped_rating'] = df['rating'].apply(map_rating)\n",
    "    df_for_ex_ai = pd.DataFrame()\n",
    "    for i in df['dataset_name'].unique():\n",
    "        num_reviews_each = 15\n",
    "        df_dataset = df[df['dataset_name']==i]\n",
    "        unique_map_rat = df_dataset['mapped_rating'].unique()\n",
    "        for c, j in enumerate(unique_map_rat):\n",
    "            samples_per_cat = num_reviews_each//(len(unique_map_rat)-c)\n",
    "            df_dataset_rat = df_dataset[df_dataset['mapped_rating']==j]\n",
    "            len_df = df_dataset_rat.shape[0]\n",
    "            to_sample = samples_per_cat\n",
    "            if len_df<samples_per_cat:\n",
    "                to_sample = len_df\n",
    "            df_for_ex_ai = df_for_ex_ai.append(df_dataset_rat.sample(to_sample))\n",
    "            num_reviews_each-=to_sample\n",
    "    \n",
    "    df_for_ex_ai.reset_index().drop(columns='index')\\\n",
    "    .to_csv('/home/user/jupyter_notebooks/df_for_ex_ai.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    del df;\n",
    "    gc.collect();\n",
    "    df_for_ex_ai = pd.read_csv('/home/user/jupyter_notebooks/df_for_ex_ai.csv')\n",
    "    df_for_ex_ai = string_to_list_dataframe(df_for_ex_ai)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb26a9-6795-41e4-913d-78c7eea4f172",
   "metadata": {},
   "source": [
    "# Load model and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c83905-591e-411c-b5d4-4712fdd9219d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,  \n",
    "                 units=128, **kwargs):\n",
    "        super(Attention,self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W1=self.add_weight(name='attention_weights_1', shape=(input_shape[-1], self.units), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        \n",
    "        self.W2=self.add_weight(name='attention_weights_2', shape=(1, self.units), \n",
    "                               initializer='glorot_uniform', trainable=True) \n",
    "        \n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0, 2, 1])\n",
    "        attention = tf.nn.softmax(tf.matmul(self.W2, tf.nn.tanh(tf.matmul(self.W1, x))))\n",
    "        weighted_context = tf.reduce_sum(x * attention, axis=-1)\n",
    "        return weighted_context, attention\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'units': self.units\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd174125-ee99-49a8-ac2f-e8eb3d756ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 10:12:43.952838: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 10:12:44.095062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1621] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14148 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:03:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/home/user/files_for_research_Vova/deep_lstm_attention_w2v_huber_sentiment.h5',\n",
    "                                  compile=False,\n",
    "                                  custom_objects={'Attention':Attention})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100472bb-3a45-4460-b333-44efc966bfc7",
   "metadata": {},
   "source": [
    "### remake model to give back attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ed9909-abdb-4b68-add2-a76e8b52e816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_model = tf.keras.models.Model(inputs=model.layers[0].input,\n",
    "                                        outputs=[[i for i in model.layers if i.name=='attention'][0].output,\n",
    "                                                 model.layers[-1].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566f2248-7287-45e0-9af8-185a06bb6229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7ff915f3dbe0>,\n",
       " <keras.layers.core.embedding.Embedding at 0x7ffad8323970>,\n",
       " <keras.layers.regularization.spatial_dropout1d.SpatialDropout1D at 0x7ffad8335940>,\n",
       " <keras.layers.rnn.lstm.LSTM at 0x7ffade53a2b0>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x7ff99870c340>,\n",
       " <keras.layers.rnn.lstm.LSTM at 0x7ff915ec6160>,\n",
       " <__main__.Attention at 0x7ff913c15520>,\n",
       " <keras.layers.core.tf_op_layer.TFOpLambda at 0x7ff913c15d90>,\n",
       " <keras.layers.core.dense.Dense at 0x7ff913c15f40>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x7ff913c15730>,\n",
       " <keras.layers.core.dense.Dense at 0x7ff913c32760>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18937d99-a802-42dd-be5b-0d335407aa9e",
   "metadata": {},
   "source": [
    "### tokenize and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbac01db-3a37-48b4-ab3f-479a1f87135a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, decoders, processors\n",
    "from tokenizers import pre_tokenizers, trainers, Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85824555-75ea-4342-8341-159eba1f6dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### because of incorrect work of BPE decoding, we should make decoding ourselves + special logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986478a3-c5c8-405c-85b1-8edbc9f19129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab, merges):\n",
    "        self.suffix = '</w>'\n",
    "        self.tokenizer = Tokenizer(models.BPE.from_file(vocab=vocab,\n",
    "            merges=merges, end_of_word_suffix=self.suffix))\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.Split(Regex(r\"[\\w'-]+|[^\\w\\s'-]+\"),'removed', True)\n",
    "        self.id_to_token = self.tokenizer.id_to_token\n",
    "        self.encode_batch = self.tokenizer.encode_batch\n",
    "        self.token_to_id = self.tokenizer.token_to_id\n",
    "        self.encode = self.tokenizer.encode\n",
    "        \n",
    "    def tokens_to_ids(self, tokens):\n",
    "        return list(map(self.token_to_id, tokens))\n",
    "    \n",
    "    def ids_to_tokens(self, ids):\n",
    "        return list(map(self.id_to_token, ids))\n",
    "        \n",
    "\n",
    "    def decode(self, tokens, return_indices=False):\n",
    "        decoded = []\n",
    "        merged_indices = []\n",
    "        i = 0\n",
    "        while i<len(tokens):\n",
    "            if tokens[i].endswith(self.suffix):\n",
    "                decoded.append(tokens[i])\n",
    "                merged_indices.append([i])\n",
    "                i+=1\n",
    "            else:\n",
    "                merged_token = ''\n",
    "                tmp_indc = []\n",
    "                while not tokens[i].endswith(self.suffix):\n",
    "                    merged_token+=tokens[i]\n",
    "                    tmp_indc.append(i)\n",
    "                    i+=1\n",
    "                merged_token+=tokens[i]\n",
    "                tmp_indc.append(i)\n",
    "                decoded.append(merged_token)\n",
    "                merged_indices.append(tmp_indc)\n",
    "                i+=1\n",
    "                \n",
    "        if return_indices:\n",
    "            return decoded, merged_indices\n",
    "        else:\n",
    "            return decoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53f053d0-c523-448f-8523-e1b315d9c929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(vocab='/home/user/files_for_research_Vova/tokenizer_30k.json',\n",
    "            merges='/home/user/files_for_research_Vova/merges_tokenizer.txt'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84dafca3-afd6-4840-9029-59f0ea2fe155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = df_for_ex_ai['review_translate'].apply(lambda x: [i.ids for i in tokenizer.encode_batch(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814dc92d-8f5a-4559-bac0-ba18a420c5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "padded = []\n",
    "for i in encoded.values:\n",
    "    padded.append(tf.keras.preprocessing.sequence.pad_sequences(i, maxlen=300,\n",
    "                                                 padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6db21506-e2e7-4cff-b94b-3efb943006a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d749b06-b44e-46e7-941a-c8b3ba39103b",
   "metadata": {},
   "source": [
    "# Attention based explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aaf3669-4c1d-4563-a740-8fa75edd6559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionExplainer:\n",
    "    def __init__(self, attention_model,\n",
    "                 tokenizer, mapping):\n",
    "        self.attention_model = attention_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mapping = mapping\n",
    "        \n",
    "    def explain(self, sample):\n",
    "        (_, attention_scores), predicted_rating = self.attention_model.predict(sample)\n",
    "        attention_scores = attention_scores.reshape(sample.shape)\n",
    "        #get only those scores which aren't relevant to PAD tokens\n",
    "        masked_scores = []\n",
    "        for i in range(attention_scores.shape[0]):\n",
    "            masked_scores.append(attention_scores[i][sample[i]!=0])\n",
    "        #actual process\n",
    "        final_scores = []\n",
    "        final_tokens = []\n",
    "        for c,sequence in enumerate(sample):\n",
    "            #map back indices to tokens\n",
    "            sequence = [i for i in sequence if i!=0]\n",
    "            tokens = self.tokenizer.ids_to_tokens(sequence)\n",
    "            #get merges of tokens\n",
    "            decoded, merged_indices = self.tokenizer.decode(tokens, True)\n",
    "            #sum up attention scores for indices which are merged\n",
    "            if len(tokens)!=len(decoded):\n",
    "                tmp_scores = []\n",
    "                for i in merged_indices:\n",
    "                    if len(i)>1:\n",
    "                        tmp_scores.append(sum([masked_scores[c][j] for j in i]))\n",
    "                    else:\n",
    "                        tmp_scores.append(masked_scores[c][i[0]])\n",
    "            else:\n",
    "                tmp_scores = list(masked_scores[c])\n",
    "            #get rid of suffix at the end of token\n",
    "            decoded = [i.rstrip(tokenizer.suffix) for i in decoded]\n",
    "            final_scores.append(tmp_scores)\n",
    "            final_tokens.append(decoded)\n",
    "        return final_scores, final_tokens, list(map(lambda x: self.mapping.get(x),\n",
    "                                                    np.argmax(predicted_rating, axis=-1)))\n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91c5131a-07b7-4e6d-96b6-ee1e310e375e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention_explainer = AttentionExplainer(attention_model, tokenizer,\n",
    "                                        mapping=inverse_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f21162f3-d55d-47b9-9c97-29d190cd3d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'positive', 1: 'negative', 2: 'neutral'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "096ddd52-3d9d-4b2f-a4a9-9e69f3d0ef49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 22\n",
    "sample = padded[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7490aa2d-5946-45c3-b843-979196ec48f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 10:12:50.130407: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA RTX A4000\" frequency: 1560 num_cores: 48 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"12000\" } environment { key: \"cudnn\" value: \"8700\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 102400 memory_size: 14835253248 bandwidth: 448064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-05-21 10:12:50.544167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 10:12:50.791043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:648] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "scores, tokens, ratings = attention_explainer.explain(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c509d-a7c4-455b-b920-a8dc5fb74512",
   "metadata": {},
   "source": [
    "### agregate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e28f1259-ce04-4e7d-a4e1-1c627a4ccdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.65.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df509004-ad69-4956-8b89-073be9051c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from functools import partial\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba3519e7-6d74-4e50-9d05-75f78ab690d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def counter_average(dict_values, counter):\n",
    "    new_dict = {}\n",
    "    for k, v in counter.items():\n",
    "        new_dict[k] = dict_values[k]/v\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bc2764d-aaa1-4a86-860c-e44f95f09bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dicts_addition(dicts):\n",
    "    overall_dict = {}\n",
    "    for dict_ in dicts:\n",
    "        for k,v in dict_.items():\n",
    "            score = overall_dict.get(k, 0)\n",
    "            overall_dict[k] = score+v\n",
    "    return overall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b31b4658-7f69-46cb-a433-42e558219535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens_scores, n_gram=2, func_agg='mean'):\n",
    "    new_tokens_scores = []\n",
    "    for token_score in tokens_scores:\n",
    "        n_gram_token_score = [token_score[i:i+n_gram] \\\n",
    "                              for i in range(len(token_score)-(n_gram-1))]\n",
    "        n_gram_token_score = [(' '.join([j[0] for j in i]), sum([j[1] for j in i]))\\\n",
    "        for i in n_gram_token_score]\n",
    "        if func_agg=='mean':\n",
    "            n_gram_token_score = [(i[0], i[1]/n_gram) for i in n_gram_token_score]\n",
    "        new_tokens_scores.append(n_gram_token_score)\n",
    "    return new_tokens_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e191509-4408-4132-bae0-fd13d6311455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def similarity_function(phrase_i, phrase_j, diversify=2):\n",
    "    sub_phrases = [' '.join(phrase_i[i:i+diversify]) for i in range(len(phrase_i)-diversify)]\n",
    "    phrase_j = ' '.join(phrase_j)\n",
    "    for i in sub_phrases:\n",
    "        if i in phrase_j:\n",
    "            return 1.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a27a7ee-7fdb-45d2-a5ef-8bdca5df05c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar(aggregated_results, diversify=2):\n",
    "    new_results = {}\n",
    "    local_similarity_function = partial(similarity_function, diversify=diversify)\n",
    "    for k, v in tqdm(aggregated_results.items()):\n",
    "        tmp_results = {}\n",
    "        phrases = [i[0].split() for i in v]\n",
    "        scores = [i[1] for i in v]\n",
    "        not_to_use = []\n",
    "        for i in range(len(phrases)):\n",
    "            if i not in not_to_use:\n",
    "                group = [i]\n",
    "                for j in range(i+1, len(phrases)):\n",
    "                    if j not in not_to_use:\n",
    "                        if local_similarity_function(phrases[i], phrases[j]) or \\\n",
    "                        local_similarity_function(phrases[j], phrases[i]):\n",
    "                            group.append(j)\n",
    "                    \n",
    "            idx, score = max(list(zip(list(group), [scores[i] for i in group])), key=lambda x: x[1])\n",
    "            not_to_use.extend(group)\n",
    "            not_to_use.remove(idx)\n",
    "            tmp_results[' '.join(phrases[idx])] = score\n",
    "        new_results[k] = list(tmp_results.items())\n",
    "        \n",
    "                   \n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c7c318-549b-40b7-96a5-c8c072c23257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_results(scores, tokens, ratings, \n",
    "                      n_gram=1,\n",
    "                      func_agg_n_gram='mean',\n",
    "                     func_agg_overall='mean', diversify=2, top_n=None):\n",
    "    tokens_scores = [list(zip(i,scores[c])) for c, i in enumerate(tokens)]\n",
    "    if n_gram>1:\n",
    "        tokens_scores = generate_ngrams(tokens_scores, n_gram, func_agg_n_gram)\n",
    "    aggregated_results = {}\n",
    "    for c, rating in enumerate(ratings):\n",
    "        tmp_dict = {}\n",
    "        tmp_counter = Counter()\n",
    "        for token, score in tokens_scores[c]:\n",
    "            local_score = tmp_dict.get(token, 0)\n",
    "            local_score+=score\n",
    "            tmp_dict[token] = local_score\n",
    "            tmp_counter.update([token])\n",
    "                \n",
    "        previous_dict, previous_counter = aggregated_results.get(rating, [{}, Counter()]) \n",
    "        aggregated_results[rating] = [dicts_addition([tmp_dict,previous_dict]), \\\n",
    "                                      previous_counter+tmp_counter]\n",
    "    if func_agg_overall=='mean':\n",
    "        aggregated_results = dict([(k, sorted(counter_average(v[0], v[1]).items(),\n",
    "                                   key=lambda x: x[1], reverse=True)) \\\n",
    "                                                 for k, v in aggregated_results.items()])\n",
    "    else:\n",
    "        aggregated_results = dict([(k, sorted(v[0].items(),\n",
    "                                             key=lambda x: x[1], reverse=True)) \\\n",
    "                                                 for k, v in aggregated_results.items()],\n",
    "                                  )\n",
    "        \n",
    "    if diversify>0 and n_gram>1:\n",
    "        aggregated_results = find_similar(aggregated_results, diversify)\n",
    "            \n",
    "        \n",
    "    if top_n:\n",
    "        aggregated_results = dict([(k, v[:top_n]) for k, v in aggregated_results.items()])\n",
    "        \n",
    "    return aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eec60268-d2c2-4adf-822c-29525f3d1f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positive': [('оптимальне співвідношення ціни послуг і', 0.08653780445456505),\n",
       "  ('до площі ринок . оптимальне', 0.05582828931510449),\n",
       "  ('багато свіжих і варених овочів', 0.055652818828821185),\n",
       "  ('сніданки смачні ) кухня задоволена', 0.05457168184220791),\n",
       "  ('смачна їжа і розваги .', 0.04855610802769661),\n",
       "  ('готель . . . привітний', 0.04664623234421015),\n",
       "  ('стіл » мав смачну традиційну', 0.044821974635124204),\n",
       "  ('ванних кімнатах . незважаючи на', 0.044567475095391273),\n",
       "  ('чудове місце . дуже смачна', 0.044522494077682495),\n",
       "  ('додатковий сніданок « шведський стіл', 0.04435810558497906),\n",
       "  ('драматичному театрі . можна прогулятися', 0.03660809826105833),\n",
       "  ('цього разу я зробив бронювання', 0.036013517528772354),\n",
       "  ('чиста і мила . стіни', 0.034280309453606606),\n",
       "  ('свіжих фруктів і солодощів .', 0.033968716487288476),\n",
       "  ('швидким . було кілька віфі', 0.033401680365204814),\n",
       "  ('готель працює дуже давно ,', 0.03303254339843988),\n",
       "  ('. в готелі середній сніданок', 0.03046175017952919),\n",
       "  ('бути , ми не вибагливі', 0.029919756576418877),\n",
       "  ('вибагливі , але нам все', 0.02877622339874506),\n",
       "  ('досить тихо . різноманітний сніданок', 0.027717598527669907)],\n",
       " 'neutral': [('рекомендується . я був щасливий', 0.05962202176451683),\n",
       "  ('номер був функціональним більше всього', 0.04774524718523025),\n",
       "  ('холодильник , санвузол . персонал', 0.04687476307153702),\n",
       "  ('. решітки важкодоступні і знаходяться', 0.038780429493635894),\n",
       "  ('. сніданок включений у вартість', 0.03834008947014809),\n",
       "  ('готельний сервіс трагічний . найгірше', 0.03392165522091091),\n",
       "  ('персонал приємний та корисний .', 0.03342168480157852),\n",
       "  ('ліжко , телевізор , холодильник', 0.027047573681920768),\n",
       "  ('найгірше - ресторан готелю .', 0.026026611123234032),\n",
       "  ('щасливий , що вже виїжджаю', 0.023272791504859926),\n",
       "  ('грубіянять і весь персонал десь', 0.022557043004781008),\n",
       "  ('. готель старий , поруч', 0.02144491272047162),\n",
       "  ('. цифри не відповідають заявленій', 0.02099461778998375),\n",
       "  ('радянські часи . заклад добре', 0.02079337202012539),\n",
       "  ('біско . номери чисті ,', 0.01861653234809637),\n",
       "  ('світі , але якість сніданку', 0.017130717798136175),\n",
       "  ('кімната димна ! ! !', 0.015991948917508124),\n",
       "  ('він оновлювався поштучно , він', 0.015205655200406908),\n",
       "  ('увагу . однозначно не рекомендується', 0.014324847818352282),\n",
       "  ('добре розташоване на самому початку', 0.01364213777706027)],\n",
       " 'negative': [('старий готель у львові ,', 0.11667753085494041),\n",
       "  ('жахливий готель . оформлення здійснюється', 0.11571026407182217),\n",
       "  ('не рекомендую , на жаль', 0.06596598811447621),\n",
       "  ('ріжуть . вилки не колються', 0.06203514682129026),\n",
       "  ('готелі шведський стіл . настала', 0.044078126549720764),\n",
       "  ('приїжджайте в цей готель !', 0.043046099692583085),\n",
       "  ('слабкий . ножі не ріжуть', 0.03392148166894913),\n",
       "  ('колються . у термосі воду', 0.028783342242240904),\n",
       "  ('настала черга приймати їжу ,', 0.024767210800200702),\n",
       "  ('мізерна . сніданок в готелі', 0.024005826469510792),\n",
       "  ('ніяких переваг немає , не', 0.023995232954621314),\n",
       "  ('на сніданок ми можемо оцінити', 0.022965471260249615),\n",
       "  ('якісний , важко описати ,', 0.0184764813631773),\n",
       "  (\"кошмар . об'їхавши всю україну\", 0.01694579585455358),\n",
       "  ('номери дуже застаріли . на', 0.014828412630595266),\n",
       "  ('багатьох . це просто жахливий', 0.012058118640561588),\n",
       "  ('втомлений від життя . ну', 0.012023651506751776),\n",
       "  ('такий жахливий . не приїжджайте', 0.010944589623250067),\n",
       "  ('крім того , що готель', 0.010418258514255286),\n",
       "  ('розташований в центрі , ніяких', 0.010227140597999096)]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_results(scores, tokens, ratings, n_gram=5, \n",
    "                  func_agg_n_gram='mean',\n",
    "                  func_agg_overall='mean',\n",
    "                  diversify=2,\n",
    "                 top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35608b2-e2e9-4db5-ab08-8f84da995b85",
   "metadata": {},
   "source": [
    "# LIME based explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "722611cf-d480-48fe-9b17-76a8c404286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from lime) (3.6.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lime) (1.22.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lime) (1.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from lime) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.8/dist-packages (from lime) (0.24.2)\n",
      "Collecting scikit-image>=0.12\n",
      "  Downloading scikit_image-0.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lazy_loader>=0.1\n",
      "  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Collecting networkx>=2.8\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (22.0)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2023.4.12-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.4/219.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime) (9.3.0)\n",
      "Collecting imageio>=2.4.1\n",
      "  Downloading imageio-2.28.1-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (1.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283858 sha256=4bb005fd314d09a9d56108d0db08ba6fedc964c20b4a4e871bccdf640c0ed054\n",
      "  Stored in directory: /root/.cache/pip/wheels/e6/a6/20/cc1e293fcdb67ede666fed293cb895395e7ecceb4467779546\n",
      "Successfully built lime\n",
      "Installing collected packages: tifffile, scipy, PyWavelets, networkx, lazy_loader, imageio, scikit-image, lime\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.6.3\n",
      "    Uninstalling scipy-1.6.3:\n",
      "      Successfully uninstalled scipy-1.6.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.6.3\n",
      "    Uninstalling networkx-2.6.3:\n",
      "      Successfully uninstalled networkx-2.6.3\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.28.1 lazy_loader-0.2 lime-0.2.0.1 networkx-3.1 scikit-image-0.20.0 scipy-1.9.1 tifffile-2023.4.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "212ff5d2-3beb-4205-baae-bcff3e84cfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_proba(arr_text, model, tokenizer, max_len, batch_size):\n",
    "    #processing\n",
    "    encoded = [i.ids for i in tokenizer.encode_batch(arr_text)]\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(encoded, maxlen=max_len,\n",
    "                                                 padding='post')\n",
    "    #prediction\n",
    "    pred=model.predict(padded, batch_size=batch_size)\n",
    "   \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb9f2d01-77d5-4a08-bb89-426de516d2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LimeExplainer:\n",
    "    def __init__(self, model, tokenizer, mapping, max_len=300, batch_size=512):\n",
    "        self.mapping = mapping\n",
    "        self.n_classes = len(self.mapping)\n",
    "        self.predict_proba = partial(predict_proba, model=model, tokenizer=tokenizer, max_len=max_len,\n",
    "                                    batch_size=batch_size)\n",
    "        self.explainer = LimeTextExplainer(class_names=self.mapping.keys(),\n",
    "                                          bow=False, feature_selection='none',\n",
    "                                          random_state=0)\n",
    "    \n",
    "    def explain(self, text_instances):\n",
    "        all_tokens = []\n",
    "        all_scores = []\n",
    "        all_ratings = []\n",
    "        for text in text_instances:\n",
    "            #use same tokenization as in lime\n",
    "            tokens = re.findall(r'\\w+', text.lower())\n",
    "            order = dict(zip(tokens, list(range(len(tokens)))))\n",
    "            ratings = []\n",
    "            scores = []\n",
    "            explanation = self.explainer.explain_instance(text, self.predict_proba,\n",
    "                                                         top_labels= self.n_classes)\n",
    "            for k, v in self.mapping.items():\n",
    "                result = explanation.as_list(k)\n",
    "                result = sorted(result, key=lambda x: order[x[0]])\n",
    "                result = [i[1] for i in result]\n",
    "                all_scores.append(result)\n",
    "                all_tokens.append(tokens)\n",
    "                all_ratings.append(v)\n",
    "                \n",
    "        return all_scores, all_tokens, all_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23385575-7876-4621-919b-2a687e6be314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lime_explainer = LimeExplainer(model, tokenizer, inverse_mapping, 300, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40c2da8c-893c-4ef1-90d7-288485a9b0de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 10:13:22.508645: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA RTX A4000\" frequency: 1560 num_cores: 48 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"12000\" } environment { key: \"cudnn\" value: \"8700\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 102400 memory_size: 14835253248 bandwidth: 448064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 23ms/step\n",
      "10/10 [==============================] - 0s 16ms/step\n",
      "10/10 [==============================] - 0s 30ms/step\n",
      "10/10 [==============================] - 0s 21ms/step\n",
      "10/10 [==============================] - 0s 17ms/step\n",
      "10/10 [==============================] - 0s 24ms/step\n",
      "10/10 [==============================] - 0s 17ms/step\n",
      "10/10 [==============================] - 0s 27ms/step\n",
      "10/10 [==============================] - 0s 31ms/step\n",
      "10/10 [==============================] - 0s 23ms/step\n",
      "10/10 [==============================] - 0s 21ms/step\n",
      "10/10 [==============================] - 0s 19ms/step\n",
      "10/10 [==============================] - 0s 19ms/step\n",
      "10/10 [==============================] - 0s 22ms/step\n",
      "10/10 [==============================] - 0s 24ms/step\n",
      "10/10 [==============================] - 0s 20ms/step\n",
      "10/10 [==============================] - 0s 34ms/step\n",
      "10/10 [==============================] - 0s 20ms/step\n",
      "10/10 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "scores, tokens, ratings = lime_explainer.explain(df_for_ex_ai['review_translate'].values[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5122816-e821-4cca-bbaf-93ad010bb016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:23<00:00,  7.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positive': [('хоча готель забезпечує гарне співвідношення',\n",
       "   0.12519681744998432),\n",
       "  ('готель привітний недорогий дуже прийнятним', 0.05854074227737462),\n",
       "  ('ніцца в центрі львова поруч', 0.055947368065229444),\n",
       "  ('приємний та корисний сніданок включений', 0.053826947905699155),\n",
       "  ('ціни та якості та чудове', 0.050382773744758705),\n",
       "  ('біско номери чисті щодня прибираються', 0.039274018283593656),\n",
       "  ('як п ятизірковий а для', 0.036816829440217747),\n",
       "  ('ресторанів від центру де багато', 0.03670783888634066),\n",
       "  ('успішно ремонтується і оновлюється чистота', 0.03527431121777858),\n",
       "  ('програма дуже мізерна сніданок в', 0.03240301921042543),\n",
       "  ('вестибюль портьє в уніформі вона', 0.02984100162307257),\n",
       "  ('мила стіни готелю всередині прикрашені', 0.02888329030396184),\n",
       "  ('багато розваг затишний нормальний готель', 0.028813288215554532),\n",
       "  ('старовинних вуличок як храму так', 0.028673751196617227),\n",
       "  ('відремонтовану кімнату але все одно', 0.02750716512583335)],\n",
       " 'negative': [('центрі ніяких переваг немає не', 0.1320024938573751),\n",
       "  ('важко описати для тварин такий', 0.12106421848359752),\n",
       "  ('навіть гаряча вода персонал не', 0.08732504121068405),\n",
       "  ('середньому як шість не рекомендую', 0.08550438117262021),\n",
       "  ('обстановка і номери дуже застаріли', 0.0711969658702673),\n",
       "  ('сервіс трагічний найгірше ресторан готелю', 0.05337270359739832),\n",
       "  ('настала черга приймати їжу їжа', 0.04953289271415581),\n",
       "  ('сніданок підходить і якісний важко', 0.0428876993897266),\n",
       "  ('дуже старий готель у львові', 0.04118578839195954),\n",
       "  ('помідори трохи старого хліба масло', 0.036017710699393767),\n",
       "  ('стін і нових вхідних дверей', 0.035238155877308386),\n",
       "  ('в готелі шведський стіл настала', 0.034000759909361106),\n",
       "  ('в готель через турфірму обов', 0.030618762928920747),\n",
       "  ('теж приходили під візит нічних', 0.02830819327073051),\n",
       "  ('сніданок ми можемо оцінити його', 0.025551967304297752)],\n",
       " 'neutral': [('прибрали номери не дуже великі', 0.07756523077786383),\n",
       "  ('довгий незручний для глядачів не', 0.07441380631871987),\n",
       "  ('більш високого рівня як підсумок', 0.07108064718268574),\n",
       "  ('самому за доступною ціною готель', 0.06556078028192361),\n",
       "  ('біско номери чисті щодня прибираються', 0.06159774470460466),\n",
       "  ('слабка новорічна програма дуже мізерна', 0.05573519684105355),\n",
       "  ('бути питанням часу та грошей', 0.05048733016187033),\n",
       "  ('в центрі відмінне скрізь біско', 0.04407326608888094),\n",
       "  ('що повернулася в радянські часи', 0.04209716932478953),\n",
       "  ('прекрасне дикун в готелі середній', 0.041605382515887024),\n",
       "  ('готель блищить не як п', 0.03988755415684525),\n",
       "  ('його в середньому як шість', 0.0387558126864983),\n",
       "  ('якщо ви відповідно оцінюєте свої', 0.03814045392108802),\n",
       "  ('тому його трансформували і прибрали', 0.03188791354364129),\n",
       "  ('код доступу до мережі wi', 0.031287921745095974)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_results(scores, tokens, ratings, n_gram=5, \n",
    "                  func_agg_n_gram='mean',\n",
    "                  func_agg_overall='mean',\n",
    "                  diversify=2,\n",
    "                 top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38438b38-6ed5-4022-a938-6b39c1fdd045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cuda12",
   "language": "python",
   "name": "tensorflow_cuda12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
