{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94761116-1530-448d-a1f6-593d90bec639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, Regex\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada8fbb-9dc2-4e9d-935d-2cb0d6684738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kim_cnn_more_layers_word_tokenizer+w2v_embeddings'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08dfac-6a47-4787-bcad-dbf254b1e234",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a021ca-f5ca-4145-aec7-4af5b56b772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format('ubercorpus.lowercased.tokenized.word2vec.300d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46fb49-08a8-4809-b697-680813299d47",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ad050-f023-4253-bc33-72c76bce3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../analysis/processed_data.csv', usecols=['review_translate_sentences_tokens',\n",
    "                                                            'dataset_name',\n",
    "                                                            'rating',\n",
    "                                                           'translated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb4540-3e6b-4e5d-a744-73cf3a50a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(x):\n",
    "    try:\n",
    "        return list(ast.literal_eval(x))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def string_to_list_dataframe(df):\n",
    "    columns = df.columns.tolist()\n",
    "    columns_w_lists = []\n",
    "    for column in columns:\n",
    "        if df[column].astype(str). \\\n",
    "                apply(lambda x: x.startswith('[') and x.endswith(']')) \\\n",
    "                .astype(int).mean() > 0.8:\n",
    "            columns_w_lists.append(column)\n",
    "    for column in columns_w_lists:\n",
    "        df[column] = df[column].apply(lambda x: str_to_list(x))\n",
    "        df = df[~df[column].isna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee8e3a-2281-4ec1-8107-49e3ba47d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = string_to_list_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b0de2-18ef-42ce-9bd5-1ff68b8a1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = pd.read_csv('../analysis/train_val_test_indices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fb1c0-5396-480b-8ce5-e728d06fc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb145e0a-2bf5-4004-b334-15433414769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = subsets.merge(df[['dataset_name', 'translated']], left_on='index', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c13af-5e8f-49e4-b337-7b3a70163c1c",
   "metadata": {},
   "source": [
    "# Encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69c9c8-b050-4904-8e93-5e1b89361f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e913ba-749b-4161-a2db-0cb6af2b025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6750f-8ed1-48c7-9d03-6c5fadb82271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = df['review_translate_sentences_tokens'].apply(lambda x: [len(i) for i in x]).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae03e8-3bd7-4c74-9e5f-92812227d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = list(chain(*sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c552ca-8570-4f18-a98a-4b7b96f779c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f4a20-1834-47ec-b6a1-dcbf9efc0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['review_translate_sentences_tokens'].apply(lambda x: list(chain(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a63040-cfba-42e0-b6f3-b9be80cfce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: [i.lower() for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9b3ad-d012-4572-8a37-adc84fce526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df['tokens'].apply(len), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ff9bc-fda5-474c-9bab-d4b295d73d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(chain(*df['tokens'].values.tolist()))\n",
    "vocab = dict([(c+1, i) for c, i in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bd2a5-b6b6-4063-9712-f736b15d447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4778f44-01be-4b49-80aa-daa54f61be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = dict([(v, k) for k,v in vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3c79b-f39c-41aa-9933-1a52ba6f1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c44fd1-112f-49a1-929b-32b51bec206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoded'] = df['tokens'].apply(lambda x: [inverse_vocab[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680a61a-cee2-4fbd-9a97-17a9c7ec74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens = df['encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6e285-87e6-4ce1-be8a-cb614270a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c9fac-7ed8-45c3-8114-763b1ed0c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokens = tf.keras.preprocessing.sequence\\\n",
    ".pad_sequences(encoded_tokens, maxlen=300, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137187b-fd7f-4199-883e-7ff778a94c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c38bc-a389-4d95-9c31-ca98edaa93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.get_vector('фучся', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5a4b2-8b08-4407-89e7-f7cd670c08b2",
   "metadata": {},
   "source": [
    "# Gather embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae92ab6-cbd3-4f2d-a447-5348124e9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_embeddings(vocab, embeddings):\n",
    "    new_embeddings = np.random.uniform(size=(len(vocab), embeddings.vector_size))\n",
    "    for k,v in vocab.items():\n",
    "        try:\n",
    "            vector = embeddings[v]\n",
    "            new_embeddings[k] = vector\n",
    "        except:\n",
    "            pass\n",
    "    return new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c266fa2-dbd7-4c97-8337-aca23a357e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gather_embeddings(vocab, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3d33e-f103-4207-aad0-899aea663e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc168c87-2e5e-4aff-afba-69e7a9a6f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9a5b3-3794-4ab8-a16d-fa572ac9d2f6",
   "metadata": {},
   "source": [
    "# Get labels and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb3b33-cbb6-4024-b683-8819ad6d7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict([(i,c) for c,i in enumerate(df['rating'].unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9dcd18-b69c-4b71-8f83-85f750be1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['rating'].map(mapping).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0a14a-7050-4689-806f-dbe4f094f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5a397-faf0-43be-b498-a3d4af06fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices, test_indices = subsets[subsets['split']=='train'].index.tolist(),\\\n",
    "subsets[subsets['split']=='val'].index.tolist(),\\\n",
    "subsets[subsets['split']=='test'].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564a2e1-11d7-44cf-803b-03b4eb3b0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y, val_y, test_y = y[train_indices], y[val_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adcfe4-0446-4673-bbad-dfdc972fb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, test_x = padded_tokens[train_indices], padded_tokens[val_indices],\\\n",
    "padded_tokens[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce92ef9-7a5c-4f99-86e0-18544e93101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0ac0e-11a7-436a-9fd3-cfa9229d7ec8",
   "metadata": {},
   "source": [
    "# Create  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a00a2-4b18-4523-a1d4-6286b9f3a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_window = 3\n",
    "n_grams_num = [3, 4, 5, 7, 9, 11]\n",
    "output_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91941772-c544-43ae-bf45-b08d249c49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "input_layer = tf.keras.layers.Input(shape=(300,), name='input')\n",
    "word_embedding = tf.keras.layers.Embedding(input_dim=len(vocab),\n",
    "                                                   output_dim=300,\n",
    "                                                   trainable=True,\n",
    "                                           name='embedding',\n",
    "                                           mask_zero=True,\n",
    "                                           weights=[embeddings]\n",
    "                                                   )\n",
    "spat_drop = tf.keras.layers.SpatialDropout1D(0.1, name='spatial_dropout')\n",
    "relu = tf.keras.layers.ReLU(name='relu')\n",
    "concat = []\n",
    "embedded = spat_drop(word_embedding(input_layer))\n",
    "for c,i in enumerate(n_grams_num):\n",
    "    conv1d = tf.keras.layers.Conv1D(filters=32, kernel_size=i, activation=None,\n",
    "                                   name=f'conv_ngram_{i}')\n",
    "    max_pooling = tf.keras.layers.MaxPool1D(pool_size=pool_window, strides=1,\n",
    "                                           padding='valid')\n",
    "    dropout = tf.keras.layers.Dropout(0.1, name=f'dropout_cnn_{c}')\n",
    "    concat.append(dropout(max_pooling(relu(conv1d(embedded)))))\n",
    "\n",
    "x = tf.keras.layers.concatenate(concat, axis=1, name='concat')\n",
    "x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu', name='dense_512')(x)\n",
    "x = tf.keras.layers.Dropout(0.3, name='dropout')(x)\n",
    "output = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "model = tf.keras.Model(input_layer, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df15e13-921d-4eaa-b433-7500dc153a5f",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272e87d-a23a-4940-81fa-569451e2ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \\\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa74f7-21cf-42b1-83fa-bb1bed688e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db9424-2b20-4d10-8da4-0c55152abe57",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9801fe-9cd2-47e2-9cad-58efbf4205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, mode='min'):\n",
    "        assert mode in ['min','max'], 'Mode should be min or max'\n",
    "        self.mode = operator.lt if mode=='min' else operator.gt \n",
    "        self.tolerance = tolerance\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.extremum_value = None\n",
    "        self.best_model = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def copy_model(model):\n",
    "        copied_model = tf.keras.models.clone_model(model)\n",
    "        copied_model.set_weights(model.get_weights())\n",
    "        return copied_model\n",
    "        \n",
    "    def __call__(self, val, model):\n",
    "        if self.extremum_value is None:\n",
    "            self.extremum_value = val\n",
    "            self.best_model = self.copy_model(model)\n",
    "        else:\n",
    "            if not self.mode(val, self.extremum_value):\n",
    "                self.counter+=1\n",
    "            else:\n",
    "                self.extremum_value = val\n",
    "                self.best_model = self.copy_model(model)\n",
    "                self.counter = 0\n",
    "        \n",
    "        if self.counter==self.tolerance:\n",
    "            self.early_stop=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676e473-8116-488e-990d-dc7937d9119c",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6de2d-be45-45cd-af31-b9042d1a20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8fc41-6661-4a56-ad5d-d1a4ed3e53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_datasets(y_true, y_pred, split='val'):\n",
    "    d = {}\n",
    "    for dataset_name in subsets['dataset_name'].unique():\n",
    "            idx = subsets[subsets['split']==split].copy()\n",
    "            idx['index'] = list(range(idx.shape[0]))\n",
    "            idx = idx[(idx['dataset_name']==dataset_name)]\\\n",
    "            ['index'].values.tolist()\n",
    "            score = f1_score(y_true=y_true[idx], y_pred=y_pred[idx],\n",
    "                                 average='micro')\n",
    "            print(f'{split} f1 score for dataset {dataset_name} : {score}')\n",
    "            d[f'{split}_f1_{dataset_name}'] = score\n",
    "            \n",
    "    for flag in [True, False]:\n",
    "        idx = subsets[subsets['split']==split].copy()\n",
    "        idx['index'] = list(range(idx.shape[0]))\n",
    "        idx = idx[idx['translated']==flag]['index'].values.tolist()\n",
    "        score = f1_score(y_true=y_true[idx], y_pred=y_pred[idx],\n",
    "                                 average='micro')\n",
    "        print(f'{split} f1 score for translated=={flag} : {score}')\n",
    "        d[f'{split}_f1_translated=={flag}'] = score\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c7824-1d7a-4fc1-b095-53902f428b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, d):\n",
    "    for key, value in d.items():\n",
    "        res = history.get(key, [])\n",
    "        res.append(value)\n",
    "        history[key] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c90c92-f6c9-4f00-94f0-582ebdf71912",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(mode='max', tolerance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4354a40-79f8-4991-a26e-6d2422a4b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, epochs=10, batch_size=128):\n",
    "    dict_history = {}\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #train model\n",
    "        history = model.fit(train_x, train_y, validation_data=(val_x, val_y), \n",
    "          epochs=1, batch_size=batch_size,\n",
    "                           verbose=0)\n",
    "        train_loss, val_loss = history.history['loss'][-1], history.history['val_loss'][-1]\n",
    "        \n",
    "        #evaluate model\n",
    "        train_prediction = np.argmax(model.predict(train_x), axis=-1)\n",
    "        val_prediction = np.argmax(model.predict(val_x), axis=-1)\n",
    "        train_f1 = f1_score(y_true=train_y, y_pred=train_prediction,\n",
    "                           average='micro')\n",
    "        val_f1 = f1_score(y_true=val_y, y_pred=val_prediction,\n",
    "                         average='micro')\n",
    "        \n",
    "        #printing evaluation\n",
    "        print(f'Epoch {i}')\n",
    "        print(f'Overall train f1 : {train_f1}, overall val f1: {val_f1}')\n",
    "        print(f'Train loss : {train_loss}, val loss: {val_loss}')\n",
    "        d_train = evaluate_on_datasets(y_true=train_y, y_pred=train_prediction, split='train')\n",
    "        d_val = evaluate_on_datasets(y_true=val_y, y_pred=val_prediction, split='val')\n",
    "            \n",
    "        if i!=epochs-1:\n",
    "            print('-'*30)\n",
    "            \n",
    "        #save history\n",
    "        update_history(dict_history, d_train)\n",
    "        update_history(dict_history, d_val)\n",
    "        update_history(dict_history, {'train_f1': train_f1})\n",
    "        update_history(dict_history, {'val_f1': val_f1})\n",
    "        update_history(dict_history, {'train_loss': train_loss})\n",
    "        update_history(dict_history, {'val_loss': val_loss})\n",
    "        #early stopping\n",
    "        \n",
    "        early_stopping(val_f1, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print('Stopping early')\n",
    "            model = early_stopping.best_model\n",
    "            break\n",
    "        \n",
    "    return dict_history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7d40c-fd2c-41cc-9938-ee4f58a8a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_history, model = \\\n",
    "training_loop(model, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16eb41-29c6-443e-95a6-b263a76a65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d77413-4c00-4295-b7db-fe06dd7b23b5",
   "metadata": {},
   "source": [
    "# Show charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8ef02-623e-4ebb-8e1d-8da77c3ed9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cf7cf-e1b2-42ce-8e95-5a2e61440247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(dict_history, columns):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in columns:\n",
    "        to_plot = dict_history[i]\n",
    "        plt.plot(range(len(to_plot)), to_plot, 'o-')\n",
    "    plt.xticks(range(len(to_plot)), range(len(to_plot)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ca35a-2166-4552-bc23-67c6bfe314d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(dict_history, ['val_loss', 'train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09978e6c-1975-4fe9-b6ed-064e00c1809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(dict_history, ['val_f1', 'train_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763d7cb-3c09-425e-984f-7675f7fae0e9",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7e54e-5868-4379-8456-c3c5ef648c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.argmax(model.predict(test_x), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc061fbf-e0a9-40be-8cb7-e356aef090d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1 = f1_score(y_true=test_y, y_pred=test_predictions,\n",
    "                         average='micro')\n",
    "print(f'Overall test f1-score : {test_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4badc-8c22-4bdc-8fb2-f7f68dbb56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_on_datasets(y_true=test_y, y_pred=test_predictions,split='test')\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d9836-a430-411b-a133-de53fbd58dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f37d8b-ee40-4e45-b937-3f602481e496",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02aeab-6be7-43e5-9273-7b2094affa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(dict_history)\n",
    "for k,v in test_results.items():\n",
    "    history[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cea36-7a81-4278-b505-d8d3e272a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history['model'] = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a68d06-e7c1-4595-bb88-3568745ad290",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.to_csv(\"training_results.csv\", mode='a', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727ce95-e7bd-4efb-a4ac-2c7d5f477068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
