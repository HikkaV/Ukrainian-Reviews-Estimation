{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94761116-1530-448d-a1f6-593d90bec639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, Regex\n",
    "import tokenizers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada8fbb-9dc2-4e9d-935d-2cb0d6684738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kim_cnn+batch_norm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46fb49-08a8-4809-b697-680813299d47",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ad050-f023-4253-bc33-72c76bce3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../analysis/processed_data.csv', usecols=['review_translate',\n",
    "                                                            'dataset_name',\n",
    "                                                            'rating',\n",
    "                                                           'translated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee8e3a-2281-4ec1-8107-49e3ba47d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b0de2-18ef-42ce-9bd5-1ff68b8a1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = pd.read_csv('../analysis/train_val_test_indices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fb1c0-5396-480b-8ce5-e728d06fc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb145e0a-2bf5-4004-b334-15433414769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = subsets.merge(df[['dataset_name', 'translated']], left_on='index', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35025fc-9c2c-490e-9231-10f4b38e230b",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ca142-1e37-49c2-b8b8-2d5f74ee98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"../analysis/tokenizer_30k.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c13af-5e8f-49e4-b337-7b3a70163c1c",
   "metadata": {},
   "source": [
    "# Encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69c9c8-b050-4904-8e93-5e1b89361f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e913ba-749b-4161-a2db-0cb6af2b025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda1c9a-9961-4097-bd69-3e5bd58d6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_translate'] = df['review_translate'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151fe97-eded-408c-be0d-df2b49051e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoded'] = tokenizer.encode_batch(df['review_translate'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01d676-5e85-4b7d-b7c0-5d99b8ba1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoded'] = df['encoded'].apply(lambda x: x.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ddda71-f140-49f7-89c8-fea94b8bcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log10(df['encoded'].apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a0c0b-0dd0-4437-872e-0c13fc041b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(df['encoded'].apply(len), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680a61a-cee2-4fbd-9a97-17a9c7ec74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens = df['encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6e285-87e6-4ce1-be8a-cb614270a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c9fac-7ed8-45c3-8114-763b1ed0c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokens = tf.keras.preprocessing.sequence\\\n",
    ".pad_sequences(encoded_tokens, maxlen=300, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137187b-fd7f-4199-883e-7ff778a94c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9a5b3-3794-4ab8-a16d-fa572ac9d2f6",
   "metadata": {},
   "source": [
    "# Get labels and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb3b33-cbb6-4024-b683-8819ad6d7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict([(i,c) for c,i in enumerate(df['rating'].unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9dcd18-b69c-4b71-8f83-85f750be1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['rating'].map(mapping).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0a14a-7050-4689-806f-dbe4f094f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5a397-faf0-43be-b498-a3d4af06fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices, test_indices = subsets[subsets['split']=='train'].index.tolist(),\\\n",
    "subsets[subsets['split']=='val'].index.tolist(),\\\n",
    "subsets[subsets['split']=='test'].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564a2e1-11d7-44cf-803b-03b4eb3b0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y, val_y, test_y = y[train_indices], y[val_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adcfe4-0446-4673-bbad-dfdc972fb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, test_x = padded_tokens[train_indices], padded_tokens[val_indices],\\\n",
    "padded_tokens[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce92ef9-7a5c-4f99-86e0-18544e93101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0ac0e-11a7-436a-9fd3-cfa9229d7ec8",
   "metadata": {},
   "source": [
    "# Create  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a00a2-4b18-4523-a1d4-6286b9f3a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_max = 5\n",
    "n_grams_min = 3\n",
    "pool_window = 2\n",
    "output_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91941772-c544-43ae-bf45-b08d249c49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "input_layer = tf.keras.layers.Input(shape=(300,), name='input')\n",
    "word_embedding = tf.keras.layers.Embedding(input_dim=tokenizer.get_vocab_size(),\n",
    "                                                   output_dim=300,\n",
    "                                                   trainable=True,\n",
    "                                           name='embedding',\n",
    "                                           mask_zero=True\n",
    "                                                   )\n",
    "relu = tf.keras.layers.ReLU(name='relu')\n",
    "concat = []\n",
    "embedded = word_embedding(input_layer)\n",
    "for i in range(n_grams_min, n_grams_max+1):\n",
    "    conv1d = tf.keras.layers.Conv1D(filters=32, kernel_size=i, activation=None,\n",
    "                                   name=f'conv_ngram_{i}')\n",
    "    max_pooling = tf.keras.layers.MaxPool1D(pool_size=pool_window, strides=1,\n",
    "                                           padding='valid')\n",
    "    batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    concat.append(max_pooling(relu(batch_norm(conv1d(embedded)))))\n",
    "\n",
    "x = tf.keras.layers.concatenate(concat, axis=1, name='concat')\n",
    "x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "x = tf.keras.layers.Dropout(0.5, name='dropout')(x)\n",
    "output = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "model = tf.keras.Model(input_layer, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df15e13-921d-4eaa-b433-7500dc153a5f",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272e87d-a23a-4940-81fa-569451e2ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \\\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db9424-2b20-4d10-8da4-0c55152abe57",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9801fe-9cd2-47e2-9cad-58efbf4205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, mode='min'):\n",
    "        assert mode in ['min','max'], 'Mode should be min or max'\n",
    "        self.mode = operator.lt if mode=='min' else operator.gt \n",
    "        self.tolerance = tolerance\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.extremum_value = None\n",
    "        self.best_model = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def copy_model(model):\n",
    "        copied_model = tf.keras.models.clone_model(model)\n",
    "        copied_model.set_weights(model.get_weights())\n",
    "        return copied_model\n",
    "        \n",
    "    def __call__(self, val, model):\n",
    "        if self.extremum_value is None:\n",
    "            self.extremum_value = val\n",
    "            self.best_model = self.copy_model(model)\n",
    "        else:\n",
    "            if not self.mode(val, self.extremum_value):\n",
    "                self.counter+=1\n",
    "            else:\n",
    "                self.extremum_value = val\n",
    "                self.best_model = self.copy_model(model)\n",
    "                self.counter = 0\n",
    "        \n",
    "        if self.counter==self.tolerance:\n",
    "            self.early_stop=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676e473-8116-488e-990d-dc7937d9119c",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6de2d-be45-45cd-af31-b9042d1a20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8fc41-6661-4a56-ad5d-d1a4ed3e53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_datasets(y_true, y_pred, split='val'):\n",
    "    d = {}\n",
    "    for dataset_name in subsets['dataset_name'].unique():\n",
    "            idx = subsets[subsets['split']==split].copy()\n",
    "            idx['index'] = list(range(idx.shape[0]))\n",
    "            idx = idx[(idx['dataset_name']==dataset_name)]\\\n",
    "            ['index'].values.tolist()\n",
    "            score = f1_score(y_true=y_true[idx], y_pred=y_pred[idx],\n",
    "                                 average='micro')\n",
    "            print(f'{split} f1 score for dataset {dataset_name} : {score}')\n",
    "            d[f'{split}_f1_{dataset_name}'] = score\n",
    "            \n",
    "    for flag in [True, False]:\n",
    "        idx = subsets[subsets['split']==split].copy()\n",
    "        idx['index'] = list(range(idx.shape[0]))\n",
    "        idx = idx[idx['translated']==flag]['index'].values.tolist()\n",
    "        score = f1_score(y_true=y_true[idx], y_pred=y_pred[idx],\n",
    "                                 average='micro')\n",
    "        print(f'{split} f1 score for translated=={flag} : {score}')\n",
    "        d[f'{split}_f1_translated=={flag}'] = score\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c7824-1d7a-4fc1-b095-53902f428b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, d):\n",
    "    for key, value in d.items():\n",
    "        res = history.get(key, [])\n",
    "        res.append(value)\n",
    "        history[key] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c90c92-f6c9-4f00-94f0-582ebdf71912",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(mode='max', tolerance=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4354a40-79f8-4991-a26e-6d2422a4b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, epochs=10, batch_size=128):\n",
    "    dict_history = {}\n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #train model\n",
    "        history = model.fit(train_x, train_y, validation_data=(val_x, val_y), \n",
    "          epochs=1, batch_size=batch_size,\n",
    "                           verbose=0)\n",
    "        train_loss, val_loss = history.history['loss'][-1], history.history['val_loss'][-1]\n",
    "        \n",
    "        #evaluate model\n",
    "        train_prediction = np.argmax(model.predict(train_x), axis=-1)\n",
    "        val_prediction = np.argmax(model.predict(val_x), axis=-1)\n",
    "        train_f1 = f1_score(y_true=train_y, y_pred=train_prediction,\n",
    "                           average='micro')\n",
    "        val_f1 = f1_score(y_true=val_y, y_pred=val_prediction,\n",
    "                         average='micro')\n",
    "        \n",
    "        #printing evaluation\n",
    "        print(f'Epoch {i}')\n",
    "        print(f'Overall train f1 : {train_f1}, overall val f1: {val_f1}')\n",
    "        print(f'Train loss : {train_loss}, val loss: {val_loss}')\n",
    "        d_train = evaluate_on_datasets(y_true=train_y, y_pred=train_prediction, split='train')\n",
    "        d_val = evaluate_on_datasets(y_true=val_y, y_pred=val_prediction, split='val')\n",
    "            \n",
    "        if i!=epochs-1:\n",
    "            print('-'*30)\n",
    "            \n",
    "        #save history\n",
    "        update_history(dict_history, d_train)\n",
    "        update_history(dict_history, d_val)\n",
    "        update_history(dict_history, {'train_f1': train_f1})\n",
    "        update_history(dict_history, {'val_f1': val_f1})\n",
    "        update_history(dict_history, {'train_loss': train_loss})\n",
    "        update_history(dict_history, {'val_loss': val_loss})\n",
    "        #early stopping\n",
    "        \n",
    "        early_stopping(val_f1, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print('Stopping early')\n",
    "            model = early_stopping.best_model\n",
    "            break\n",
    "        \n",
    "    return dict_history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7d40c-fd2c-41cc-9938-ee4f58a8a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_history, model = \\\n",
    "training_loop(model, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16eb41-29c6-443e-95a6-b263a76a65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d77413-4c00-4295-b7db-fe06dd7b23b5",
   "metadata": {},
   "source": [
    "# Show charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8ef02-623e-4ebb-8e1d-8da77c3ed9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cf7cf-e1b2-42ce-8e95-5a2e61440247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(dict_history, columns):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in columns:\n",
    "        to_plot = dict_history[i]\n",
    "        plt.plot(range(len(to_plot)), to_plot, 'o-')\n",
    "    plt.xticks(range(len(to_plot)), range(len(to_plot)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ca35a-2166-4552-bc23-67c6bfe314d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(dict_history, ['val_loss', 'train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09978e6c-1975-4fe9-b6ed-064e00c1809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(dict_history, ['val_f1', 'train_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763d7cb-3c09-425e-984f-7675f7fae0e9",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7e54e-5868-4379-8456-c3c5ef648c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.argmax(model.predict(test_x), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc061fbf-e0a9-40be-8cb7-e356aef090d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1 = f1_score(y_true=test_y, y_pred=test_predictions,\n",
    "                         average='micro')\n",
    "print(f'Overall test f1-score : {test_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4badc-8c22-4bdc-8fb2-f7f68dbb56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluate_on_datasets(y_true=test_y, y_pred=test_predictions,split='test')\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447ad8a-3695-499c-9fe3-6d6d9cc2834c",
   "metadata": {},
   "source": [
    "# Save history results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1cb15-8dae-414b-80c9-cabbcfd4168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(dict_history)\n",
    "for k,v in test_results.items():\n",
    "    history[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc4c15-4277-4913-8080-91cddcc4d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "history['model'] = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fd96f-8e81-47b6-8332-0bb3892edfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.to_csv(\"training_results.csv\", mode='a')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
