{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8a8cb-e08e-42f7-b93b-6dba4fc17a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1370b-1618-4ff1-8cdf-a5745cffff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(x):\n",
    "    try:\n",
    "        return list(ast.literal_eval(x))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def string_to_list_dataframe(df):\n",
    "    columns = df.columns.tolist()\n",
    "    columns_w_lists = []\n",
    "    for column in columns:\n",
    "        if df[column].astype(str). \\\n",
    "                apply(lambda x: x.startswith('[') and x.endswith(']')) \\\n",
    "                .astype(int).mean() > 0.8:\n",
    "            columns_w_lists.append(column)\n",
    "    for column in columns_w_lists:\n",
    "        df[column] = df[column].apply(lambda x: str_to_list(x))\n",
    "        df = df[~df[column].isna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0bfa53-b10e-47d7-bec1-75196f325483",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d14a6-b7a4-4634-9bba-85066b7cff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d25a02-203f-49ba-b637-a9ae3c09b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = string_to_list_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e63c97-e126-4bb1-ad69-012bcfcbc63b",
   "metadata": {},
   "source": [
    "# See number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d5eb3-5546-44cb-b9dc-27b7811c6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a2922-ed43-494e-980d-a49aa0890da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(chain(*list(chain(*df['review_translate_sentences_lemma'].values.tolist())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d3161-f087-4451-811e-e8b4f10019dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(chain(*list(chain(*df['review_translate_sentences_tokens'].values.tolist())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bdfb0-a66c-49e8-a922-5955351456a3",
   "metadata": {},
   "source": [
    "# Train BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aea512-683a-4cbb-8a61-2d2ddad0738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee51b46-6366-4dd8-984f-f03473fe0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, Regex\n",
    "import tokenizers\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7936d2b4-285c-4dac-a7c2-4070b42fa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_translate'] = df['review_translate'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66b9f9-6e25-4013-8c5f-b00ac1f9d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tok = pre_tokenizers.Split(Regex(r\"[\\w'-]+|[^\\w\\s'-]+\"),'removed', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48663c8-3ecf-4c55-989b-20946c16c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = models.BPE(min_frequency=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75b2cc-1061-4c5f-afad-30058d76e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model=bpe)\n",
    "tokenizer.pre_tokenizer = pre_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc2727-b5b8-4aa0-87fc-05f7d97b0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(special_tokens\\\n",
    "                              =[\"[UNK]\", \"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c2683-7619-419d-98c8-fab7e91ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['review_translate'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19b2a9-4f45-4fe4-ba0f-c2f4e34dc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(data, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c6aad-3061-429a-9322-e7f748632e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('я ебав тебе в рот').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d59848-fd6c-49f8-a8f6-0099f208b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fa6f4-154a-452a-9dc2-f0ac0849574d",
   "metadata": {},
   "source": [
    "# Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6370e4a-f9df-40b7-bd36-1484d312cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer_30k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2a598-0aa1-44a8-8193-c9cacb1bc817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
